{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training FRED on Jean ZAY multi-GPU with Data Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-LIEPQJ9\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load tensorflow-gpu/py3/2.3.1\n",
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import horovod.tensorflow as hvd\n",
    "\n",
    "from fred import S2S, compute_loss, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size):\n",
    "    nlp = English()\n",
    "    tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "    dir=\"../../datasets/export\"\n",
    "    authors = os.listdir(dir)   \n",
    "\n",
    "    max_length=512\n",
    "    n_sentences = 200\n",
    "    data = []\n",
    "    for author in authors:\n",
    "        books=os.listdir(os.path.join(dir, author))\n",
    "        for book in books:\n",
    "            count=0\n",
    "            with open(os.path.join(dir,author, book), 'r',encoding=\"utf-8\") as fp:\n",
    "                lines=fp.readlines()\n",
    "                for line in lines[100:]:\n",
    "                    count=count+1\n",
    "                    sent=line.replace(\"\\n\",\"\")\n",
    "                    tok = ['<S>'] + [token.string.strip() for token in tokenizer(sent.lower()) if token.string.strip() != ''] + ['</S>']\n",
    "                    \n",
    "                    data.append((author,sent,tok[:512]))\n",
    "\n",
    "                    if count==n_sentences:\n",
    "                        break\n",
    "                    \n",
    "    df = pd.DataFrame(data, columns =['Author', 'Raw', 'Tokens']) \n",
    "\n",
    "    raw_data = list(df['Tokens'])\n",
    "    flat_list = [item for sublist in raw_data for item in sublist]\n",
    "    freq = FreqDist(flat_list)\n",
    "\n",
    "    print(\"Training Word2Vec\")\n",
    "\n",
    "    EMBEDDING_SIZE = 100\n",
    "    w2v = Word2Vec(list(df['Tokens']), size=EMBEDDING_SIZE, window=10, min_count=1, negative=10, workers=10)\n",
    "    word_map = {}\n",
    "    word_map[\"<PAD>\"] = 0\n",
    "    word_vectors = [np.zeros((EMBEDDING_SIZE,))]\n",
    "    for i, w in enumerate([w for w in w2v.wv.vocab]):\n",
    "        word_map[w] = i+1\n",
    "        word_vectors.append(w2v.wv[w])\n",
    "    word_vectors = np.vstack(word_vectors)\n",
    "    i2w = dict(zip([*word_map.values()],[*word_map]))\n",
    "    nw = word_vectors.shape[0]\n",
    "    na = len(df.Author.unique())\n",
    "    print(\"%d auteurs et %d mots\" % (na,nw))\n",
    "\n",
    "    ang_tok,mask_ang_tok,ang_tok_shift,mask_ang_tok_shift,ang_pl = pad([[word_map[w] for w in text] for text in raw_data],shift = True)\n",
    "\n",
    "    aut2id = {auth:i for i, auth in enumerate(df.Author.unique())}\n",
    "    authors_id = np.asarray([aut2id[i] for i in list(df['Author'])])\n",
    "    authors_id = np.expand_dims(authors_id, 1)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    D=np.load(\"use_embeddings_512_200.npy\")\n",
    "    X = np.hstack([authors_id,D,ang_tok,mask_ang_tok])\n",
    "    Y = np.hstack([ang_tok_shift,mask_ang_tok_shift])\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "    \n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Initialize Horovod\n",
    "    hvd.init()\n",
    "\n",
    "    # display info\n",
    "    if hvd.rank() == 0:\n",
    "        print(\">>> Training on \", hvd.size() // hvd.local_size(), \" nodes and \", hvd.size(), \" processes\", flush=True)\n",
    "    print(\"- Process {} corresponds to GPU {} of node {}\".format(hvd.rank(), hvd.local_rank(), hvd.rank() // hvd.local_size()), flush=True)\n",
    "    \n",
    "    # Pin GPU to be used to process local rank (one GPU per process)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "    \n",
    "    batch_size=32\n",
    "    X=build_dataset(batch_size=batch_size)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80, random_state=101)\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((X_train,Y_train)).batch(batch_size)\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((X_test,Y_test)).batch(batch_size)\n",
    "    \n",
    "    # ### Model declaration and training\n",
    "\n",
    "    print(\"Building model ... \\n\", flush=True)\n",
    "    model = S2S(na,word_vectors,i2w,ang_pl)\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    loss_f = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "    opt = tf.optimizers.Adam(learning_rate=0.01 * hvd.size())\n",
    "\n",
    "    # Horovod: add Horovod DistributedOptimizer.\n",
    "    # opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "    loss_f = tf.keras.losses.CategoricalCrossentropy()\n",
    "    epochs = 100\n",
    "\n",
    "    checkpoint_dir = 'training_checkpoints' if hvd.rank() == 0 else None\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_multi\") if hvd.rank() == 0 else None\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=opt,\n",
    "                                    model=model)\n",
    "\n",
    "    @tf.function\n",
    "    def compute_apply_gradients_multigpu(a,x_topic,x,x_mask,y,y_mask, first_batch):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            loss,label,prediction= compute_loss(model, loss_f,a,x_topic,x,x_mask,y,y_mask)\n",
    "            \n",
    "        tape = hvd.DistributedGradientTape(tape)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if first_batch:\n",
    "            hvd.broadcast_variables(model.variables, root_rank=0)\n",
    "            hvd.broadcast_variables(opt.variables(), root_rank=0)\n",
    "        \n",
    "        return loss,label,prediction\n",
    "    \n",
    "    tr_loss = []\n",
    "    te_loss = []\n",
    "    tr_acc = []\n",
    "    te_acc = []\n",
    "\n",
    "    print(\"Beginning training ... \\n\", flush=True)\n",
    "\n",
    "    start=datetime.datetime.now()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(epoch,flush=True,)\n",
    "\n",
    "        for batch, (x,y) in enumerate(tqdm(train_data.take(len(train_data) // hvd.size()))):\n",
    "            \n",
    "            a,x_topic,x,x_mask = tf.split(x,[1,512,ang_pl,ang_pl],axis=1)\n",
    "\n",
    "            y,y_mask = tf.split(y,2,axis=1)\n",
    "            \n",
    "            y = tf.one_hot(y,depth = nw)\n",
    "            \n",
    "            loss,label,prediction = compute_apply_gradients_multigpu(a,x_topic,x,x_mask,y,y_mask, batch==0)\n",
    "\n",
    "            train_loss(loss)\n",
    "            train_accuracy(label, prediction)\n",
    "\n",
    "        if (hvd.rank()==0):   \n",
    "            for x,y in tqdm(test_data):\n",
    "\n",
    "                print(\"Testing ... \\n\", flush=True)\n",
    "                a,x_topic,x,x_mask = tf.split(x,[1,512,ang_pl,ang_pl],axis=1)\n",
    "                y,y_mask = tf.split(y,2,axis=1)\n",
    "                \n",
    "                y = tf.one_hot(y,depth = nw)\n",
    "                \n",
    "                loss,label,prediction = compute_loss(model,loss_f,a,x_topic,x,x_mask,y,y_mask)\n",
    "\n",
    "                test_loss(loss)\n",
    "                test_accuracy(label, prediction)\n",
    "\n",
    "            #print(\" \".join(model.generate(aut2id[\"radiohead\"],word_map['<S>'],word_map['</S>'])))\n",
    "            #print(\" \".join(model.generate(aut2id[\"disney\"],word_map['<S>'],word_map['</S>'])))\n",
    "\n",
    "            print(\n",
    "            f'Loss: {train_loss.result()}, '\n",
    "            f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "            f'Test Loss: {test_loss.result()}, '\n",
    "            f'Test Accuracy: {test_accuracy.result() * 100}', flush=True)\n",
    "\n",
    "            with open(\"loss_results_multi.txt\", \"a\") as ff:\n",
    "                ff.write('%06f | %06f | %06f | %06f' % (train_loss.result(), test_loss.result(), train_accuracy.result()*100, test_accuracy.result()*100))\n",
    "            \n",
    "            tr_loss.append(train_loss.result())\n",
    "            te_loss.append(test_loss.result())\n",
    "            tr_acc.append(train_accuracy.result())\n",
    "            te_acc.append(test_accuracy.result())\n",
    "\n",
    "            if (epoch % 10 == 0):\n",
    "                checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    if (hvd.rank()==0):\n",
    "        print(' -- Trained in ' + str(datetime.datetime.now()-start) + ' -- ')\n",
    "        A = []\n",
    "        for i in range(model.na):\n",
    "            A.append(model.A(i).numpy())\n",
    "        A = np.vstack(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
